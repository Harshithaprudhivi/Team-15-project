{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Harshithaprudhivi/Team-15-project/blob/main/ECG_Synthesizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ByiTcxCtcw4Q",
        "outputId": "9068f933-8fb3-47c1-8af8-45b98125dbc0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting wfdb\n",
            "  Downloading wfdb-4.3.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting neurokit2\n",
            "  Downloading neurokit2-0.2.11-py2.py3-none-any.whl.metadata (37 kB)\n",
            "Collecting biosppy\n",
            "  Downloading biosppy-2.2.3-py2.py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: aiohttp>=3.10.11 in /usr/local/lib/python3.11/dist-packages (from wfdb) (3.11.15)\n",
            "Requirement already satisfied: fsspec>=2023.10.0 in /usr/local/lib/python3.11/dist-packages (from wfdb) (2025.3.2)\n",
            "Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from wfdb) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.26.4 in /usr/local/lib/python3.11/dist-packages (from wfdb) (2.0.2)\n",
            "Collecting pandas>=2.2.3 (from wfdb)\n",
            "  Downloading pandas-2.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.8.1 in /usr/local/lib/python3.11/dist-packages (from wfdb) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from wfdb) (1.15.3)\n",
            "Requirement already satisfied: soundfile>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from wfdb) (0.13.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from neurokit2) (1.6.1)\n",
            "Collecting bidict (from biosppy)\n",
            "  Downloading bidict-0.23.1-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from biosppy) (3.14.0)\n",
            "Collecting shortuuid (from biosppy)\n",
            "  Downloading shortuuid-1.0.13-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from biosppy) (1.17.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from biosppy) (1.5.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (from biosppy) (4.11.0.86)\n",
            "Requirement already satisfied: pywavelets in /usr/local/lib/python3.11/dist-packages (from biosppy) (1.8.0)\n",
            "Collecting mock (from biosppy)\n",
            "  Downloading mock-5.2.0-py3-none-any.whl.metadata (3.1 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.11->wfdb) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.11->wfdb) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.11->wfdb) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.11->wfdb) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.11->wfdb) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.11->wfdb) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.11->wfdb) (1.20.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.2.2->wfdb) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.2.2->wfdb) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.2.2->wfdb) (4.58.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.2.2->wfdb) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.2.2->wfdb) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.2.2->wfdb) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.2.2->wfdb) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.2.2->wfdb) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2.3->wfdb) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2.3->wfdb) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.8.1->wfdb) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.8.1->wfdb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.8.1->wfdb) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.8.1->wfdb) (2025.4.26)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0.0->neurokit2) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.10.0->wfdb) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.10.0->wfdb) (2.22)\n",
            "Downloading wfdb-4.3.0-py3-none-any.whl (163 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading neurokit2-0.2.11-py2.py3-none-any.whl (696 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m696.5/696.5 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading biosppy-2.2.3-py2.py3-none-any.whl (158 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.0/158.0 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bidict-0.23.1-py3-none-any.whl (32 kB)\n",
            "Downloading mock-5.2.0-py3-none-any.whl (31 kB)\n",
            "Downloading shortuuid-1.0.13-py3-none-any.whl (10 kB)\n",
            "Installing collected packages: shortuuid, mock, bidict, pandas, wfdb, neurokit2, biosppy\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.0 which is incompatible.\n",
            "dask-cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.0 which is incompatible.\n",
            "cudf-cu12 25.2.1 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed bidict-0.23.1 biosppy-2.2.3 mock-5.2.0 neurokit2-0.2.11 pandas-2.3.0 shortuuid-1.0.13 wfdb-4.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install wfdb neurokit2 biosppy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OoE88s-Oe0Ca"
      },
      "outputs": [],
      "source": [
        "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
        "import wfdb, neurokit2 as nk\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv1D, LSTM, TimeDistributed, Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5KzoDezQxgE"
      },
      "source": [
        "# New section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkY36EfSQjFc"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4DKqP03fI8z",
        "outputId": "d482f618-bb7d-46a6-b131-42e814869cf9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "from pathlib import Path\n",
        "drive.mount('/content/drive/')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oUNlX2ocfgbH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import wfdb\n",
        "import numpy as np\n",
        "\n",
        "def convert_all_wfdb_dirs(base_input_dirs, base_output_root):\n",
        "    for input_dir in base_input_dirs:\n",
        "        rel_dir_name = os.path.basename(input_dir.rstrip('/'))\n",
        "        output_dir = os.path.join(base_output_root, f'npy_{rel_dir_name}')\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        for root, _, files in os.walk(input_dir, topdown=True, followlinks=False):\n",
        "            for file in files:\n",
        "                if file.endswith('.hea'):\n",
        "                    record_path = os.path.join(root, file[:-4])\n",
        "                    rel_path = os.path.relpath(record_path, input_dir)\n",
        "                    save_path = os.path.join(output_dir, f'{rel_path}.npy')\n",
        "\n",
        "                    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "\n",
        "                    try:\n",
        "                        print(f\"Reading: {record_path}\")\n",
        "                        record = wfdb.rdrecord(record_path)\n",
        "\n",
        "                        if record.p_signal is None:\n",
        "                            print(f\"Skipping: {record_path} — p_signal is None\")\n",
        "                            continue\n",
        "\n",
        "                        if record.p_signal.nbytes > 1e9:  # >1GB, you can adjust this\n",
        "                            print(f\"Warning: {record_path} — p_signal too large to save safely\")\n",
        "                            continue\n",
        "\n",
        "                        print(f\"Saving to: {save_path}\")\n",
        "                        np.save(save_path, record.p_signal)\n",
        "                        print(f\"Saved: {save_path}\")\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"Failed: {record_path} — {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6xfXvLBmxZ4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9BDeYkKm7s5"
      },
      "outputs": [],
      "source": [
        "# Define paths to both datasets\n",
        "base_dirs = {\n",
        "    \"100\": \"/content/drive/MyDrive/PTB-data/npy_records100\",\n",
        "    \"500\": \"/content/drive/MyDrive/PTB-data/npy_records500\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8fBsZ23nIJ9"
      },
      "outputs": [],
      "source": [
        "\n",
        "BATCH_SIZE = 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WciU4qBJQrv1"
      },
      "outputs": [],
      "source": [
        "# Final merged dataset\n",
        "X_all, Y_all = [], []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15twmIEpnIMr"
      },
      "outputs": [],
      "source": [
        "def bandpass_filter(ecg): return ecg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knjbUbM7nIPh"
      },
      "outputs": [],
      "source": [
        "def normalize(ecg):\n",
        "    max_val = np.max(np.abs(ecg), axis=0, keepdims=True)\n",
        "    return ecg / np.where(max_val == 0, 1, max_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zI88ks-DnISY"
      },
      "outputs": [],
      "source": [
        "def extract_input_output(ecg): return ecg[:, :3], ecg  # I, II, V2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "WkuXOLxmQ0ry",
        "outputId": "2b1211b3-0660-4a94-bfd4-9223cdfc6cf7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🔍 Starting processing for 100 Hz folder\n",
            "✅ Found 21847 .npy files in 100 Hz folder\n",
            "\n",
            "🚀 [100Hz] Processing batch 1/22\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:50<00:00, 19.78it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Processed batch 1/22 — Shape: (1000, 1000, 3)\n",
            "\n",
            "🚀 [100Hz] Processing batch 2/22\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [04:20<00:00,  3.85it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Processed batch 2/22 — Shape: (1000, 1000, 3)\n",
            "\n",
            "🚀 [100Hz] Processing batch 3/22\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [04:21<00:00,  3.82it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Processed batch 3/22 — Shape: (1000, 1000, 3)\n",
            "\n",
            "🚀 [100Hz] Processing batch 4/22\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [04:10<00:00,  3.99it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Processed batch 4/22 — Shape: (1000, 1000, 3)\n",
            "\n",
            "🚀 [100Hz] Processing batch 5/22\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [04:14<00:00,  3.92it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Processed batch 5/22 — Shape: (1000, 1000, 3)\n",
            "\n",
            "🚀 [100Hz] Processing batch 6/22\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [04:12<00:00,  3.95it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Processed batch 6/22 — Shape: (1000, 1000, 3)\n",
            "\n",
            "🚀 [100Hz] Processing batch 7/22\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [04:16<00:00,  3.91it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Processed batch 7/22 — Shape: (1000, 1000, 3)\n",
            "\n",
            "🚀 [100Hz] Processing batch 8/22\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [04:18<00:00,  3.87it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Processed batch 8/22 — Shape: (1000, 1000, 3)\n",
            "\n",
            "🚀 [100Hz] Processing batch 9/22\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [04:13<00:00,  3.94it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Processed batch 9/22 — Shape: (1000, 1000, 3)\n",
            "\n",
            "🚀 [100Hz] Processing batch 10/22\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [04:17<00:00,  3.89it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Processed batch 10/22 — Shape: (1000, 1000, 3)\n",
            "\n",
            "🚀 [100Hz] Processing batch 11/22\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [04:06<00:00,  4.06it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Processed batch 11/22 — Shape: (1000, 1000, 3)\n",
            "\n",
            "🚀 [100Hz] Processing batch 12/22\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [04:11<00:00,  3.97it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Processed batch 12/22 — Shape: (1000, 1000, 3)\n",
            "\n",
            "🚀 [100Hz] Processing batch 13/22\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [04:13<00:00,  3.94it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Processed batch 13/22 — Shape: (1000, 1000, 3)\n",
            "\n",
            "🚀 [100Hz] Processing batch 14/22\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [04:14<00:00,  3.93it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Processed batch 14/22 — Shape: (1000, 1000, 3)\n",
            "\n",
            "🚀 [100Hz] Processing batch 15/22\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [04:10<00:00,  3.99it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Processed batch 15/22 — Shape: (1000, 1000, 3)\n",
            "\n",
            "🚀 [100Hz] Processing batch 16/22\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [04:15<00:00,  3.91it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Processed batch 16/22 — Shape: (1000, 1000, 3)\n",
            "\n",
            "🚀 [100Hz] Processing batch 17/22\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [04:08<00:00,  4.02it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Processed batch 17/22 — Shape: (1000, 1000, 3)\n",
            "\n",
            "🚀 [100Hz] Processing batch 18/22\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [04:01<00:00,  4.15it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Processed batch 18/22 — Shape: (1000, 1000, 3)\n",
            "\n",
            "🚀 [100Hz] Processing batch 19/22\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [04:04<00:00,  4.09it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Processed batch 19/22 — Shape: (1000, 1000, 3)\n",
            "\n",
            "🚀 [100Hz] Processing batch 20/22\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [04:05<00:00,  4.08it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Processed batch 20/22 — Shape: (1000, 1000, 3)\n",
            "\n",
            "🚀 [100Hz] Processing batch 21/22\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  3%|▎         | 32/1000 [00:07<04:06,  3.93it/s]"
          ]
        }
      ],
      "source": [
        "\n",
        "for label, data_folder in base_dirs.items():\n",
        "    print(f\"\\n🔍 Starting processing for {label} Hz folder\")\n",
        "\n",
        "    # Gather all .npy files\n",
        "    file_list = []\n",
        "    for root, _, files in os.walk(data_folder):\n",
        "        for f in files:\n",
        "            if f.endswith('.npy'):\n",
        "                file_list.append(os.path.join(root, f))\n",
        "    print(f\"✅ Found {len(file_list)} .npy files in {label} Hz folder\")\n",
        "\n",
        "    # Batch processing\n",
        "    num_batches = (len(file_list) + BATCH_SIZE - 1) // BATCH_SIZE\n",
        "\n",
        "    for batch_idx in range(num_batches):\n",
        "        start = batch_idx * BATCH_SIZE\n",
        "        end = min(start + BATCH_SIZE, len(file_list))\n",
        "        batch_files = file_list[start:end]\n",
        "\n",
        "        X_batch, Y_batch = [], []\n",
        "\n",
        "        print(f\"\\n🚀 [{label}Hz] Processing batch {batch_idx + 1}/{num_batches}\")\n",
        "\n",
        "        for file_path in tqdm(batch_files):\n",
        "            try:\n",
        "                ecg = np.load(file_path)\n",
        "\n",
        "                if ecg.ndim != 2 or ecg.shape[1] != 12:\n",
        "                    print(f\"⚠️ Skipped: {file_path} (Invalid shape: {ecg.shape})\")\n",
        "                    continue\n",
        "\n",
        "                ecg = normalize(bandpass_filter(ecg))\n",
        "                x, y = extract_input_output(ecg)\n",
        "\n",
        "                X_batch.append(x)\n",
        "                Y_batch.append(y)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Failed to process {file_path}: {e}\")\n",
        "\n",
        "        # Convert to arrays (stay in RAM)\n",
        "        X_batch = np.array(X_batch)\n",
        "        Y_batch = np.array(Y_batch)\n",
        "\n",
        "        print(f\"✅ Processed batch {batch_idx + 1}/{num_batches} — Shape: {X_batch.shape}\")\n",
        "\n",
        "        # 🔸 At this point, you can train or analyze on this batch in-memory\n",
        "        # Example: model.train_on_batch(X_batch, Y_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cmt5eKUXgoM6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Amy-nOwXgoJo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_J6cbiDbR8NL"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1yTnRso_2NbRxUbTpR32GAMF1WAKnFhbA",
      "authorship_tag": "ABX9TyMR6IZ8GH93LVHDc3JNBJpX",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}